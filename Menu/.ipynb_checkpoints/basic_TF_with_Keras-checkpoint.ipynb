{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a3d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d9ffe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda87dca",
   "metadata": {},
   "source": [
    "# 1.1 Loading 2D mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2124cd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "display(X_train_full.shape, X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04baf650",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(len(X_train_full))\n",
    "X_train_full, y_train_full = X_train_full[shuffle_idx], y_train_full[shuffle_idx]\n",
    "X_valid, X_train = X_train_full[: 5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1510a8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67446e55",
   "metadata": {},
   "source": [
    "# 1.2 customize a layer\n",
    "https://keras.io/guides/making_new_layers_and_models_via_subclassing/#putting-it-all-together-an-endtoend-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26433d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear layer\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units = 128):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape): # `input_shape` is a built-in variable\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape = (input_shape[-1], self.units), dtype = \"float32\"), \n",
    "            trainable = True,\n",
    "        )\n",
    "        # self.w = self.add_weight(shape = (input_shape[-1], self.units), initializer = \"random_normal\", trainable = True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value = b_init(shape = (self.units,), dtype = \"float32\"), \n",
    "            trainable = True,\n",
    "        )\n",
    "        # self.b = self.add_weight(shape = (self.units,), initializer = \"zeros\", trainable = True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4b72f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.0630539   0.05878359  0.03976886 -0.06102928]\n",
      " [-0.0630539   0.05878359  0.03976886 -0.06102928]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 3))\n",
    "linear_layer = Linear(4)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bd768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers are recursively composable\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(300)\n",
    "        self.linear_2 = Linear(100)\n",
    "        self.linear_3 = Linear(10)\n",
    "        self.Leaky = keras.layers.LeakyReLU(alpha = 0.1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.flatten_1 = keras.layers.Flatten(input_shape = [inputs.shape[-2], inputs.shape[-1]])\n",
    "        x = self.flatten_1(inputs)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.Leaky(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.Leaky(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = tf.nn.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3487ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape = (30, 28, 28)))\n",
    "print(len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee5cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer outer_layer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the `add_loss()` method\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate  = 1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs)) # reduce dim\n",
    "        return inputs\n",
    "\n",
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1\n",
    "\n",
    "_ = layer(tf.zeros(2, 2))\n",
    "len(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed14cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0090072>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(\n",
    "            32, kernel_regularizer = tf.keras.regularizers.l1_l2(1e-3, 1e-3)\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs): \n",
    "        return self.dense(inputs)\n",
    "\n",
    "r_layer = OuterLayerWithKernelRegularizer()\n",
    "_ = r_layer(tf.zeros((1, 1)))\n",
    "print(r_layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e55110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `add_metric()`, to track losses\n",
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name = None):\n",
    "        super(LogisticEndpoint, self).__init__(name = name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    def call(self, targets, logits, sample_weights = None):\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name = \"accuracy\")\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a36bfacd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'value_index' and 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-8d52369add8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ml_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticEndpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'value_index' and 'dtype'"
     ]
    }
   ],
   "source": [
    "l_layer = LogisticEndpoint()\n",
    "targets = tf.ones((2, 2))\n",
    "logits = tf.Tensor([[0, 1], [1, 1]], dtype = tf.float32)\n",
    "_ = l_layer(targets, logits)\n",
    "display(l_layer.metrics, float(l_layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79a6f2f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-07d77c1cee83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'shape'"
     ]
    }
   ],
   "source": [
    "tf.Tensor([4, 6], shape=(2,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84443656",
   "metadata": {},
   "source": [
    "# 1.3 building NN\n",
    "* easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [28, 28])) # feature dimensionality\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\")) # num of neurons\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441bc9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model.summary())\n",
    "keras.utils.plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5ef4d",
   "metadata": {},
   "source": [
    "## 1.2.1 inspect a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13eaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden0 = model.layers[0]\n",
    "display(hidden0, hidden0.name, model.get_layer(hidden0.name) is hidden0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "display(weights.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1875c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
